<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title>
	
 
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<style>
.container {
  position: relative;
}

.text-block {
  position: relative;
  top: 0px;
  right: 0px;
  margin-left: 5px;
  width: 97.5%;
  text-align: center;
  border-radius:10px 10px 0px 0px;
  border: 1px solid #787878;
  background-color: #787878;
  color: white;
  padding-left: 0px;
  padding-right: 0px;
  padding-top: 3px;
  padding-bottom: 3px;
}
</style>
</head>
<body>

<!-- <nav class="navbar" style="margin-bottom:-40px" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="index.html">
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>-->


<section class="hero">
  <div style="margin-bottom:-80px" class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</h1>
          <h1 style="font-size:2vw"><font color="#ff0000"> <b> Pre-Print </b> </font></h1> <br>
		<h1 style="font-size:2vw"><font color="#ff0000"> <b>  </b> </font></h1> <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Siminfar Samakoush Galougah</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/rishieraj/">Rishie Raj</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://schowdhury671.github.io/">Sanjoy Chowdhury</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sayannag.github.io/">Sayan Nag</a><sup>2</sup>,
            </span>
			      <span class="author-block">
              <a href="https://users.umiacs.umd.edu/~ramanid/">Ramani Duraiswami</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland,</span>
			      <span class="author-block"><sup>2</sup>Adobe Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
			  <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2508.07470"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/_CNJeiYwLsE?si=1QnqvgwbC6lq_86e"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
			        <!--  Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1ua93_3XyDAZZ6arGQij7OqAZR3UO5pZW?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rishieraj/aura.git"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
			  <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
				 -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    
	<!-- TL;DR. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
		<center>
        <div class="content has-text-justified" style='width:750px'>
          <p>
            We introduce AURA, a benchmark that forces models to reason using both audio and video, and AuraScore, a metric that evaluates if this reasoning is factually and logically sound.
            Our findings reveal a major "reasoning gap" in even the best models, which achieve high accuracy (~92%) but fail on our reasoning evaluation (~45%).
          </p>
        </div>
		</center>
      </div>
    </div>
	
</div>
</section>
    <!--/ TL;DR. -->
	
  <section class="columns is-vcentered interpolation-panel" width=100%>
  <div class="container is-max-desktop">
    <div class="hero-body" style='margin-top:-25px;margin-bottom:-25px'>
	<center>
      <!-- <h2 class="title is-3">MelFusion Framework</h2> -->
      <img src="./static/images/teaser.png"
                 class="interpolation-image" width=80%/></center>
      <p class="content has-text-justified">
        We present AURA, the first question-answering (QA) benchmark designed to evaluate state-
        of-the-art Audio-Visual Large Language Models (AV-LLMs) and Omni-Modal Language Models (OLMs) on fine-grained
        cognitive tasks, including Cross-Modal Causal Reasoning, Timbre/Pitch Reasoning, Tempo/AV Synchronization Analysis,
        Unanswerability, Implicit Distractions, and Performer Skill Profiling.
      </p>
    </div>
  </div>
  </section>

  <section class="section">
  <div class="container is-max-desktop">
    
  <!-- TL;DR. -->
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Abstract</h2>
    <center>
        <div class="hero-body">
          <p class="content has-text-justified">
            Current audio-visual (AV) benchmarks focus on final answer accuracy, overlooking the underlying reasoning process.
            This makes it difficult to distinguish genuine comprehension from correct answers derived through fallacies or hallucinations.
            To address this, we introduce <i>AURA</i> (Audio-visual Understanding and Reasoning Assessment), a benchmark for evaluating the cross-modal
            reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs).
            AURA includes questions across six challenging cognitive domains, such as causality, timbre and pitch, tempo and AV synchronization,
            unanswerability, implicit distractions, and skill profiling, explicitly designed to be unanswerable from a single modality.
            This forces models to construct a valid logical path grounded in both audio and video, setting AURA apart from AV datasets that
            allow uni-modal shortcuts. To assess reasoning traces, we propose a novel metric, <i>AuraScore</i>, which addresses the lack of robust tools
            for evaluating reasoning fidelity. It decomposes reasoning into two aspects: (i) Factual Consistency - whether reasoning is grounded
            in perceptual evidence, and (ii) Core Inference - the logical validity of each reasoning step. Evaluations of SOTA models on AURA reveal
            a critical reasoning gap: although models achieve high accuracy (up to 92% on some tasks), their Factual Consistency and
            Core Inference scores fall below 45%. This discrepancy highlights that models often arrive at correct answers through flawed logic,
            underscoring the need for our benchmark and paving the way for more robust multimodal evaluation.
		      </p>
        </div>
    </center>
      </div>
    </div>
  
</div>
</section>

<section class="columns is-vcentered interpolation-panel" width=100%>
  <div class="container is-max-desktop" >
    <div class="hero-body" style='margin-top:-25px;margin-bottom:-25px'>
    <center><h2 class="title is-3">AURA Benchmark Statistics</h2></center><br>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column" >
        <div class="content">
          <!-- <center><h1 style="font-size: 25px; color:brown" " class="title is-3"></h1></center> -->
          <img src="static/images/data_stats.png" height=100%/>
          <p class="content has-text-justified">
            <b>Dataset statistics of AURA.</b> The plots in the above figure showcase the details of the data distribution in AURA.
            (a) The number of MCQ-questions in each QA/task category. (b) Percentage distribution of video samples in the dataset based on their duration.
            (c) Contribution of video samples to the dataset based on their source of origin.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop" >
    <center><h2 class="title is-3">Automatic QA Generation Pipeline</h2></center><br>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column" >
        <div class="content">
          <!-- <center><h1 style="font-size: 25px; color:brown" " class="title is-3"></h1></center> -->
          <img src="static/images/data_pipeline.png" height=100%/>
          <p class="content has-text-justified">
            <b>The automated QA generation pipeline for the AURA benchmark.</b> The process begins with (1) Captioning and Transcription,
            where an input video is decomposed into text descriptions using specialized models. These multi-modal annotations are then inserted 
            into a (2) Prompt Template for GPT-4o. The complete prompt is then used to (3) Generate a structured JSON output containing the question,
            multiple-choice options, the correct answer, and a "gold standard" reasoning trace.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="columns is-vcentered interpolation-panel" width=100%>
  <div class="container is-max-desktop" >
    <div class="hero-body" style='margin-top:-25px;margin-bottom:-25px'>
    <center><h2 class="title is-3">Quantitative Results</h2></center><br>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column" >
        <div class="content">
          <!-- <center><h1 style="font-size: 25px; color:brown" " class="title is-3"></h1></center> -->
          <img src="static/images/eval_results.png" height=100%/>
          <p class="content has-text-justified">
            <b>Evaluation Scores for Omnimodal LLMs.</b> Our proposed AuraScore, consisting of ACC(Accuracy), FCS (Factual Consistency Score),
            CIS (Core Inference Score), provides insight into the performance of the SOTA Models across multiple tasks in AURA.
            Closed source models are at the bottom.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop" >
    <center><h2 class="title is-3">Qualitative Results</h2></center><br>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column" >
        <div class="content">
          <!-- <center><h1 style="font-size: 25px; color:brown" " class="title is-3"></h1></center> -->
          <img src="static/images/qual_exp_1.png" height=100%/>
          <img src="static/images/qual_exp_2.png" height=100%/>
          <img src="static/images/qual_exp_3.png" height=100%/>
          <p class="content has-text-justified">
            <b>Qualitative results on tasks.</b> We show several qualitative results on our six different tasks, CR, TPR, TSA, UANS, ID and PSP. 
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top:0px">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      <!-- @article{chowdhury2025avtrustbench,
  title={AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs},
  author={Chowdhury, Sanjoy and Nag, Sayan and Dasgupta, Subhrajyoti and Wang, Yaoting and Elhoseiny, Mohamed and Gao, Ruohan and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2501.02135},
  year={2025}
} -->
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template of this website is borrowed from <a
              href="https://nerfies.github.io/">nerfies</a> website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
